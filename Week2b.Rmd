---
title: "Week 2b"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Bonferroni Correction
A way to control familywise error rate even when the tests are not independent
Bonferroni designed an adjustment to prevent data from incorrectly appearing to be statistically significant.

## Bonferroni Correction Exercises
Error controlling procedure: set of instructions, such as "r
reject all the null hypothesis for for which p-values < 0.0001" or "reject the null hypothesis for the 10 features with smallest p-values".

## Bonferroni Correction Exercises #1 (Bonferroni versus Sidak)

family wide error rate (FWER):probability of incorrectly rejecting the null at least once Pr(V>0).
the probability of at least 1 false positive when multiple comparisons are being tested.

What we want to do in practice is choose a procedure that guarantees this probability is smaller than a predetermined value such as 0.05. Here we keep it general and instead of 0.05 we use alpha

We have already learned that the procedure "pick all the genes with p-value <0.05" fails miserably as we have seen that Pr(V>0)~1 . So what else can we do?

The Bonferroni procedure assumes we have computed p-values for each test and asks what constant K should we pick so that the procedure "pick all genes with p-value less than k has Pr(V>0) = 0.05. And we typically want to be conservative rather than lenient, so we accept a procedure that has Pr(V>0) <= 0.05.
So the first result we rely on is that this probability is largest when all the null hypotheses are true:
Pr(V>0) <= Pr(V>0 | all nulls are true)
or using the notation:
Pr(V>0) <= Pr(V>0 | m1=0)
In an earlier assessment we showed that if the tests are independent then
 Pr(V>0 | m1=0) = 1-(1-K)^m
 and we pick K so that 
1-(1-K)^m = alpha which give us : K= 1-(1-alpha)^(1/m)

Now, this requires the tests to be independent. The Bonferroni procedure does not make this assumption.
Sidak method assumes that each comparison is independent of the others. 
if we set K=alpha/m , this procedure has the property that Pr(V>0) <= alpha.

```{r }
alpha <- seq(0,0.25,0.01)
m = 1:10000
bonferroni = alpha/m
sidak = 1-(1 - alpha)**(1/m)
plot(m, sidak, col = "red", ylab = "k", pch = 20)
points(m, bonferroni, col = "green", pch = 21)
```

## Bonferroni Correction Exercises #2 (Monte Carlo simulation)

To simulate the p-value results of, say, 8,793 t-tests for which the null is true, we don't actual have to generate the original data. As we learned in class, we can generate p-values from a uniform distribution like this:
```{r }
pvals <- runif(8793,0,1)
```

Using what we have learned, set the cutoff using the Bonferroni correction that guarantees an FWER lower than 0.05 and report back the FWER. Set the seed at 1,set.seed(1), and run 10,000 simulations. Report the Monte Carlo estimate of the FWER below.
```{r }
set.seed(1)
B <- 10000
m <- 8793
alpha <- 0.05
pvals <- matrix(runif(B*m,0,1),B,m)
k <- alpha/m
mistakes <- rowSums(pvals<k) 
mean(mistakes>0)
```

## Bonferroni Correction Exercises #3
Using the same seed repeat the above for Sidak's cutoff.

```{r }
set.seed(1)
B <- 10000
m <- 8793
alpha <- 0.05
pvals <- matrix(runif(B*m,0,1),B,m)
k = 1-(1 - alpha)**(1/m)
mistakes <- rowSums(pvals<k) 
mean(mistakes>0)
```

## False Discovery Rate

```{r }
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv"
filename <- "femaleControlsPopulation.csv"
if (!file.exists(filename)) download(url,destfile=filename)
```

```{r }
set.seed(1)
population = unlist(read.csv("femaleControlsPopulation.csv"))
alpha <- 0.05
N <- 12
m <- 10000
p0 <- 0.90 ##10% of diets work, 90% don't
m0 <- m*p0
m1 <- m-m0
nullHypothesis <- c(rep(TRUE,m0), rep(FALSE,m1))
delta <- 3
```

```{r }
controls <- matrix(sample(population,N*m, replace=TRUE),nrow=m)
treatments <- matrix(sample(population, N*m, replace=TRUE),nrow=m)

treatments[which(!nullHypothesis),]<-
  treatments[which(!nullHypothesis),]+delta
```

combine to form a matrix
```{r }
dat <- cbind(controls,treatments)
g <- factor(c(rep(0,N),rep(1,N)))
pvals <- rowttests(dat,g)$p.value
sum(pvals <= alpha/m) #Bonferroni correction
```

2 diets out of 1000 get called significant. The probability of making a mistake is low but there are false negative and this is the only problem with Bonferroni correction

## False discovery rate
defines the expected value of the proportion of features that you call significant that are incorrect, that are type one errors. And we try to minimize that rate. 
```{r }
calls <- pvals <= alpha
R <- sum(calls)
V <- sum(nullHypothesis & calls) #false positive, null hypothesis is true
Q <- ifelse(R>0, V/R, 0) #quantity, expextation related to FDR
```

## By requiring a FWR < 0.05, we are practically assuring 0 power
```{r }
set.seed(1)
B <- 1000
Qs <- replicate(B,{
  ##matrix with control data (rows are tests, columns are mice)
  controls <- matrix(sample(population, N*m, replace=TRUE),nrow=m)
  
  ##matrix with control data (rows are tests, columns are mice)
  treatments <-  matrix(sample(population, N*m, replace=TRUE),nrow=m)
  ##add effect to 10% of them
  treatments[which(!nullHypothesis),]<-treatments[which(!nullHypothesis),]+delta
  
  ##combine to form one matrix
  dat <- cbind(controls,treatments)
  
 calls <- rowttests(dat,g)$p.value < alpha
 R=sum(calls)
 Q=ifelse(R>0,sum(nullHypothesis & calls)/R,0)
 return(Q)
})
```

# Controlling FDR
```{r }
library(rafalib)
mypar(1,1)
hist(Qs)
FDR <- mean(Qs)
print(FDR)
```
The FDR is relatively high here. This is because for 90% of the tests, the null hypotheses is true. This implies that with a 0.05 p-value cut-off, out of 100 tests we incorrectly call between 4 and 5 significant on average. This combined with the fact that we don't "catch" all the cases where the alternative is true, gives us a relatively high FDR

#### Benjamini-Hochberg (Advanced)
We want to construct a procedure that guarantees the FDR to be below a certain level
A way to control the false discovery rate.
Instead of controlling FWER as the Bonferroni, we control the FDR

```{r }
alpha <- 0.05
B <- 1000 ##number of simulations. We should increase for more precision
res <- replicate(B,{
  controls <- matrix(sample(population, N*m, replace=TRUE),nrow=m)
  treatments <-  matrix(sample(population, N*m, replace=TRUE),nrow=m)
  treatments[which(!nullHypothesis),]<-treatments[which(!nullHypothesis),]+delta
  dat <- cbind(controls,treatments)
  pvals <- rowttests(dat,g)$p.value 
  ##then the FDR
  calls <- p.adjust(pvals,method="fdr") < alpha
  R=sum(calls)
  Q=ifelse(R>0,sum(nullHypothesis & calls)/R,0)
  return(c(R,Q))
})
Qs <- res[2,]
mypar(1,1)
hist(Qs) ##Q is a random variable, this is its distribution
FDR=mean(Qs)
print(FDR)
```


## Direct Approach to FDR
We are no longer going to set a alpha-level before the experiment. 
Instead, we assume some list will be created: R>0
Focus on estimating FDR for that list

## q-values
We compute a q-value for each feature 
If a feature resulted in a p-value of p, the q-value is the estimated pFDR for a list of all features with a p-value <= p
The estimated pi0 can be used to improve the estimate of pFDR

## FDR Exercises

```{r }
library(devtools)
library(rafalib)
install_github("genomicsclass/GSE5859Subset")
BiocManager::install(c("genefilter", "qvalue"))
```

## FDR Exercises #1

Load the gene expression data:
```{r }
library(GSE5859Subset)
data(GSE5859Subset)
```

We are interested in comparing gene expression between the two groups defined in the sampleInfo table.

Compute a p-value for each gene using the function rowttests() from the genefilter package in Bioconducto
```{r }
library(genefilter)
g <- factor(sampleInfo$group)
pvals = rowttests(geneExpression,g)$p.value
sum(pvals<0.05)
```
## FDR Exercises #2
Apply the Bonferroni correction to the p-values obtained in question #1 to achieve a FWER of 0.05. How many genes are called significant under this procedure?
```{r }
k = 0.05/length(pvals)
sum(pvals<k)
```
Note that we went from over a thousand to just 10. One practical question we should ask is "are we being too conservative?". Note that we have not really assessed false negatives at all yet.


## FDR Exercises #3

Note that the FDR is a property of a list of features, not each specific feature. The q-value relates FDR to an individual feature. To define the q-value we order features we tested by p-value then compute the FDRs for a list with the most significant, the two most significant, the three most significant, etc... The FDR of the list with the, say, m most significant tests is defined as the q-value of the m-th most significant feature. In other words, the q-value of a feature, is the FDR of the biggest list that includes that gene.

In R, we can compute the q-value using the p.adjust function with the FDR option. Read the help file for p.adjust and then, for our gene expression dataset, compute how many genes achieve an FDR < 0.05
```{r }
fdr = p.adjust(pvals,method="fdr")
sum(fdr<0.05)
```

Note that controlling the FDR at 0.05 gives us 3 more genes than the Bonferroni correction. Note that we are controlloing two very different error rates. Here we are saying that we think this list of 13 genes has about 5% false positives. The Bonferroni procedure gave us a list ot 10 genes for which we were quite certain had no false positives. Note again that we have not discussed false negatives.

## FDR Exercises #4

Now use the qvalue function, in the Bioconductor qvalue package, to estimate q-values using the procedure described by Storey.

Using this estimate how many genes have q-values below 0.05
```{r }
library(qvalue)
res = qvalue(pvals)
qvals = res$qvalues
sum(qvals<0.05)
```
Now the list has increased to 17. However, we are also claiming this list has an FDR of 5%. The reason this list is different is because as explained in the videos, qvalue, estimates FDR differently and is less conservative. Remember that the theory provides bounds for FDR: it guarantees FDR will be less than 0.05. If qvalue does in fact estimate pi0 well then it will provide a list with FDR closer to 0.05.

## FDR Exercises #5

Read the help file for qvalue and report the estimated proportion of genes for which the null hypothesis is true
pi = m0/m
```{r }
qvalue(pvals)$pi0
```

## FDR Exercises #6

Note that we have the number of genes passing the q-value <0.05 threshold is larger with the qvalue function than the p.adjust difference.

Why is this the case? Make a plot of the ratio of these two estimates to help answer the question.
```{r }
plot(qvalue(pvals)$qvalue/p.adjust(pvals,method="fdr"))
abline(h=qvalue(pvals)$pi0,col=2)
```
To get an idea of how pi0 is estimated, note that if we look at the histogram, pi0 roughly tells us the proportion that looks about uniform:
```{r }
hist(pvals,breaks=seq(0,1,len=21))
expectedfreq <- length(pvals)/20 #per bin
abline(h=expectedfreq*qvalue(pvals)$pi0,col=2,lty=2)
```
## FDR Exercises #7

Create a Monte Carlo Simulation in which you simulate measurements from 8,793 genes for 24 samples: 12 cases and 12 controls.
```{r }
n <- 24
m <- 8793
mat <- matrix(rnorm(n*m),m,n)
```

Now for 500 genes, there is a difference of 2 between cases and controls:
```{r }
delta <- 2
positives <- 500
mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
```

So the null hypothesis is true for 8793-500 genes. Using the notation from the videos m=8793, m0=8293 and m1=500

Set the seed at 1, set.seed(1), and run this experiment 1,000 times with a Monte Carlo simulation. For each instance compute p-values using a t-test (using rowttests() in the genefilter package) and create three lists of genes using:

Bonferroni correction to achieve an FWER of 0.05,
p.adjust() estimates of FDR to achieve an FDR of 0.05, and
qvalue() estimates of FDR to to achieve an FDR of 0.05.
For each of these three lists compute the number of false positives in the list and the number of false negatives: genes not in the list that should have been because the null hypothesis is not true (we added 2 to the controls to create the cases).

What is the false positive rate (false positives divided by m0) if we use Bonferroni?
```{r }
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <- 2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##
  FP1 <- sum(pvals[-(1:positives)]<= 0.05/m)
  FP1
})
```

Now to get the false positive rate we divide the false positives by the total number of genes for which the null hypothesis is true:
```{r }
mean(result/(m-positives))
```
Note that this value is much smaller than 0.05. This is because Bonferroni controls FWER to be 0.05 not the FDR. In this case, controlling FWER to be 0.05 gives us very low FDR. This makes intuitive sense since having just 1 mistake out of 8,293 possible mistakes is very small and we trying to avoid even 1.


## FDR Exercises #8

From the same Monte Carlo simulation as in the question above, what is the false negative rate if we use Bonferroni?
```{r }
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
  c(FP1,FN1)
  })
mean(result[2,]/(positives))
```

## FDR Exercises #9

From the same Monte Carlo simulation as in question #7, what is the false positive rate if we use q-values from p.adjust?
```{r }
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
  #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  c(FP1,FN1,FP2)
  })
# Then to get our false positive rate
mean(result[3,]/(m-positives))
```
Note that although much higher than the FDR for Bonferroni, the FDR is substantially lower than 0.05 we were shooting for. This is because the Benjamini–Hochberg procedure gives us a bound. The larger m1, the more conservative this approximation will be.


## FDR Exercises #10
From the same Monte Carlo simulation as in question #7, what is the false negative rate if we use q-values from p.adjust?
```{r }
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
   #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  FN2 <- sum(qvals1[1:positives]>0.05)
  c(FP1,FN1,FP2,FN2)
  })
  
#Then our false negative rate is
mean(result[4,]/(positives))
```
Here we see the potential advantage of FDR over FWER, in particular if our goal is discovery. The false negative rate is much reduced now from 0.76 to 0.08


## FDR Exercises #11

From the same Monte Carlo simulation as in question #7, what is the false positive rate if we use q-values from qvalue function?
```{r }
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
   #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  FN2 <- sum(qvals1[1:positives]>0.05)
  #qvalue q-value
  qvals2 = qvalue(pvals)$qvalue
  FP3 <- sum(qvals2[-(1:positives)]<=0.05)
  c(FP1,FN1,FP2,FN2,FP3)
  })
  
#Then the false positive rate is
mean(result[5,]/(m-positives))
```
Here we see that by estimating pi0 this approach gets closer to the targeted FDR of 0.05.


## FDR Exercises #12
From the same Monte Carlo simulation as in question #7, what is the false negative rate if we use q-values from qvalue function?
```{r }
set.seed(1)
library(qvalue)
library(genefilter)
n <- 24
m <- 8793
B <- 1000
delta <-2
positives <- 500
g <- factor(rep(c(0,1),each=12))
result <- replicate(B,{
  mat <- matrix(rnorm(n*m),m,n)
  mat[1:positives,1:(n/2)] <- mat[1:positives,1:(n/2)]+delta
  pvals = rowttests(mat,g)$p.val
  ##Bonferroni
  FP1 <- sum(pvals[-(1:positives)]<=0.05/m)  
  FN1 <- sum(pvals[1:positives]>0.05/m)
   #p.adjust q-value
  qvals1 = p.adjust(pvals,method="fdr")
  FP2 <- sum(qvals1[-(1:positives)]<=0.05)
  FN2 <- sum(qvals1[1:positives]>0.05)
  #qvalue q-value
  qvals2 = qvalue(pvals)$qvalue
  FP3 <- sum(qvals2[-(1:positives)]<=0.05)
  FN3 <- sum(qvals2[1:positives]>0.05)  
  c(FP1,FN1,FP2,FN2,FP3,FN3)
  })
  
#Then our false negative rate is
mean(result[6,]/(positives))
```
Here we see that by creating a list of an FDR closer to 0.05 we are less conservative and thus decrease the false negative rate further.