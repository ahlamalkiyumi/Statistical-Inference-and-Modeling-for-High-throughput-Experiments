---
title: "Week 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Procedures 
A procedure is a recipe for which results we call significant.
Procedure are made flexible through parameters or cutoffs.

Example of procedure for a high-throughput experiment:
-Pick an alpha
-Compute a p-value for each feature
-Call significant all features with p-values smaller than alpha

## Completely Null model
We will use a Monte Carlo simulation using our mice data to imitate a situation in which we perform tests for 10,000 different fad diets, none of them having an effect on weight. 

```{r }
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleControlsPopulation.csv"
filename <- "femaleControlsPopulation.csv"
if (!file.exists(filename)) download(url,destfile=filename)
```

```{r }
set.seed(1)
population = unlist(read.csv("femaleControlsPopulation.csv"))
alpha <- 0.05
N <- 12
m <- 10000
pvals <- replicate(m,{
  control = sample(population,N)
  treatment = sample(population,N)
  t.test(treatment,control)$p.value
})
sum(pvals < 0.05)
```

Now alternative is true for 10%
10% of the diets are effective
```{r }
alpha <- 0.05
N <- 12
m <- 10000
p0 <- 0.90 ##10% of diets work, 90% don't
m0 <- m*p0
m1 <- m-m0
nullHypothesis <- c(rep(TRUE,m0), rep(FALSE,m1))
delta <- 3
```

## Create the simulation and perform a t-test on each
```{r }
set.seed(1)
calls <- sapply(1:m, function(i){
  control <- sample(population,N)
  treatment <- sample(population,N)
  if(!nullHypothesis[i]) treatment <- treatment + delta
  ifelse( t.test(treatment,control)$p.value < alpha, 
          "Called Significant",
          "Not Called Significant")
})

```


Because in this simulation we know the truth (saved in `nullHypothesis`), we can compute the entries of the table:
```{r }
null_hypothesis <- factor(nullHypothesis, levels=c("TRUE","FALSE"))
table(null_hypothesis, calls)
```
We see from the table type I error and type II error. 
The alternative is true but 447 of them we didn't call significant. 

If we run the simulation repeatedly, these values change. 
```{r }
B <- 10 ##number of simulations
VandS <- replicate(B,{
  calls <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val < alpha
  })
  cat("V =",sum(nullHypothesis & calls), "S =",sum(!nullHypothesis & calls),"\n")
  c(sum(nullHypothesis & calls),sum(!nullHypothesis & calls))
  })
```
## Error Rates and Procedures Exercises #1

```{r }
population <- read.csv('femaleControlsPopulation.csv') %>% unlist()
set.seed(1)
N <- 12
B <- 10000
pvals <- replicate(B,{
  control = sample(population,N)
  treatment = sample(population,N)
  t.test(treatment,control)$p.val 
})
mypar(1,2)
hist(pvals)
plot(ecdf(pvals)) # identity line
```

## Error Rates and Procedures Exercises #2

We previously learned that under the null, the probability of a p-value < p is p. If we run 8,793 independent tests, what is the probability of incorrectly rejecting at least one of the null hypotheses?
1 - (1-k)^8793 using k=0.05
```{r }
1 - 0.95^8793
```

## Error Rates and Procedures Exercises #3 (Sidak's procedure)

Suppose we need to run 8,793 statistical tests and we want to make the probability of a mistake very small, say 5%. Using the answer to exercise #2, how small do we have to change the cutoff, previously 0.05, to lower our probability of at least one mistake to be 5%.

```{r }
m <- 8793
1 - 0.95^(1/m)
```

## Vectorizing code

To give an example of how we can simulate  and  we constructed a simulation with:
```{r }
alpha <- 0.05
N <- 12
m <- 10000
p0 <- 0.90 #10% of diets work, 90% don't
m0 <- m*p0
m1 <- m-m1
nullHypothesis <- c(rep(TRUE,m0), rep(FALSE,m1))
delta <- 3
```

We then ran a Monte Carlo simulation by repeating a procedure in which 10,000 tests were run one by one using sapply().
```{r }
B <- 10 ##number of simulations 
system.time(
VandS <- replicate(B,{
  calls <- sapply(1:m, function(i){
    control <- sample(population,N)
    treatment <- sample(population,N)
    if(!nullHypothesis[i]) treatment <- treatment + delta
    t.test(treatment,control)$p.val < alpha
  })
  c(sum(nullHypothesis & calls),sum(!nullHypothesis & calls))
  })
)
```

In each iteration we checked if that iteration was associated with the null or alternative hypothesis. We did this with the line
if(!nullHypothesis[i]) treatment <- treatment + delta
In R, operations based on matrices are typically much faster than operations performed within loops or sapply(). We can vectorize the code to make it go much faster. This means that instead of using sapply() to run m tests, we will create a matrix with all data in one call to sample.

This code runs several times faster than the code above, which is necessary here due to the fact that we will be generating several simulations. Understanding this chunk of code and how it is equivalent to the code above using sapply() will take a you long way in helping you code efficiently in R.

```{r }
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("genefilter")
```

```{r }
library(genefilter) ##rowttests is here
set.seed(1)
##Define groups to be used with rowttests
g <- factor( c(rep(0,N),rep(1,N)) )
B <- 10 ##number of simulations
system.time(
VandS <- replicate(B,{
  ##matrix with control data (rows are tests, columns are mice)
  controls <- matrix(sample(population, N*m, replace=TRUE),nrow=m)
  
  ##matrix with control data (rows are tests, columns are mice)
  treatments <-  matrix(sample(population, N*m, replace=TRUE),nrow=m)
  
  ##add effect to 10% of them
  treatments[which(!nullHypothesis),]<-treatments[which(!nullHypothesis),]+delta
  
  ##combine to form one matrix
  dat <- cbind(controls,treatments)
  
 calls <- rowttests(dat,g)$p.value < alpha
 
  c(sum(nullHypothesis & calls),sum(!nullHypothesis & calls))
})
)
```

```{r }

```
